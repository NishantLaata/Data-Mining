# Data-Mining
Q1: 
import numpy as np
import pandas as pd
from pandas_profiling import ProfileReport
import pandas_profiling
from pandas_profiling import ProfileReport

df = pd.DataFrame(np.random.rand(100, 5), columns=["a", "b", "c", "d", "e"])

#Q1(a) python 
file = pd.read_csv(r'C:\Users\W10\Desktop\is 733 dATAMINING\Homework 2\red_wine.csv')
print(file)

profile = ProfileReport(file)

profile.to_notebook_iframe()



# Q2 Naive bayes

import pandas as pd
from sklearn.naive_bayes import GaussianNB
from sklearn import metrics
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import accuracy_score


def import_data():
    # import total dataset
    data = pd.read_csv(r'C:\Users\W10\Desktop\is 733 dATAMINING\Homework 2\red_wine.csv')
   
    # get a list of column names
    headers = list(data.columns.values)
   
    # separate into independent and dependent variables
    x = data[headers[:-1]]
    y = data[headers[-1:]].values.ravel()

    return x, y

if __name__ == '__main__':
    # get training and testing sets
    x, y = import_data()

    # set to 10 folds
    skf = StratifiedKFold(n_splits=10)

    # blank lists to store predicted values and actual values
    predicted_y = []
    expected_y = []
    acc_score = []
    # partition data
    for train_index, test_index in skf.split(x, y):
        # specific ".loc" syntax for working with dataframes
        x_train, x_test = x.loc[train_index], x.loc[test_index]
        y_train, y_test = y[train_index], y[test_index]

        # create and fit classifier
        classifier = GaussianNB()
        classifier.fit(x_train, y_train)

        # store result from classification
        predicted_y.extend(classifier.predict(x_test))

        # store expected result for this specific fold
        expected_y.extend(y_test)

    # save and print accuracy
        acc = metrics.accuracy_score(expected_y, predicted_y)*100
        acc_score.append(acc)
       
    avg_acc_score = (sum(acc_score)/10)
 
   
    print('accuracy of each fold - {}'.format(acc_score))
    print('Avg accuracy : {}'.format(avg_acc_score))
    
    
    import numpy as np
    from sklearn import metrics
    y = np.array([1, 1, 2, 2])
    pred = np.array([0.2, 0.6, 0.35, 0.8])
    
    fpr, tpr, thresholds = metrics.roc_curve(y, pred, pos_label=2)
    metrics.auc(fpr, tpr)
    print(metrics.auc(fpr, tpr))
    
    
    
    # Q2 Logisitc regession
# Logistic regression
#Importing required libraries
import numpy as np

import pandas as pd
from sklearn.model_selection import KFold
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
 
#Loading the dataset
df = pd.read_csv(r'C:\Users\W10\Desktop\is 733 dATAMINING\Homework 2\red_wine.csv')

X = df.iloc[:,:-1]
y = df.iloc[:,-1]
 
#Implementing cross validation
 
k = 10
kf = KFold(n_splits=k, random_state=None)
model = LogisticRegression(solver= 'liblinear')
 
acc_score = []
 
for train_index , test_index in kf.split(X):
    X_train , X_test = X.iloc[train_index,:],X.iloc[test_index,:]
    y_train , y_test = y[train_index] , y[test_index]
     
    model.fit(X_train,y_train)
    pred_values = model.predict(X_test)
     
    acc = accuracy_score(pred_values , y_test)*100
    acc_score.append(acc)
     
avg_acc_score = (sum(acc_score)/k)
 
print('accuracy of each fold - {}'.format(acc_score))
print('Avg accuracy : {}'.format(avg_acc_score))


from sklearn.metrics import roc_auc_score
clf = LogisticRegression(solver="liblinear", random_state=0).fit(X, y)
roc_auc_score(y, clf.predict_proba(X)[:, 1])
roc_auc_score(y, clf.decision_function(X))


# 2 decsion tree
# Load libraries
import pandas as pd
from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier
from sklearn.model_selection import train_test_split # Import train_test_split function
from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation

# from sklearn.metrics import roc_curve, roc_auc_score
# from matplotlib import pyplot as plt
# from sklearn import tree
# from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
# from sklearn.datasets import make_regression


# X = df.iloc[:,:-1]
# y = df.iloc[:,-1]
 
    
# pima = pd.read_csv(r'C:\Users\W10\Desktop\is 733 dATAMINING\Homework 2\red_wine.csv')

from sklearn.datasets import make_classification
from sklearn import datasets
from sklearn.datasets import fetch_openml
mice = fetch_openml(name = 'red_wine',version=2)
import numpy as np
import sklearn
# from sklearn.datasets import fetch_rcv1
# rcv1 = fetch_rcv1()


X = mice.data
type_map = {'high': 1, 'low': 0}
mice.data['type'] = mice.data['type'].map(type_map)
Y = mice.data.pop('type')



# Split dataset into training set and test set
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=1) # 80% training and 20% test



# Create Decision Tree classifer object
clf = DecisionTreeClassifier()

# Train Decision Tree Classifer
clf = clf.fit(X_train,y_train)

#Predict the response for test dataset
y_pred = clf.predict(X_test)

# Model Accuracy, how often is the classifier correct?
print("Accuracy:",(metrics.accuracy_score(y_test, y_pred))*100)


from sklearn.metrics import roc_curve, auc
model = DecisionTreeClassifier()
probs = model.fit(X_train, y_train).predict_proba(X_test)
fpr, tpr, threshold = roc_curve(y_test, probs[:,1])
print("Decision Tree Area under curve -> ",auc(fpr, tpr))

# Q3 ROC random forest

from sklearn.datasets import make_classification
from sklearn import datasets
from sklearn.datasets import fetch_openml
mice = fetch_openml(name = 'red_wine',version=2)
import numpy as np
import sklearn
# from sklearn.datasets import fetch_rcv1
# rcv1 = fetch_rcv1()


X = mice.data
type_map = {'high': 1, 'low': 0}
mice.data['type'] = mice.data['type'].map(type_map)
Y = mice.data.pop('type')

print(X, Y)

# X, Y = make_classification(n_samples=571, n_classes=4, n_features=2, random_state=0)

# print(X, Y)

from sklearn.model_selection import train_test_split


X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.2,
                                                    random_state=0)
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB

rf = RandomForestClassifier(max_features=3, n_estimators=6)
rf.fit(X_train, Y_train)


r_probs = [0 for _ in range(len(Y_test))]
rf_probs = rf.predict_proba(X_test)

rf_probs = rf_probs[:, 1]

from sklearn.metrics import roc_curve, roc_auc_score

r_auc = roc_auc_score(Y_test, r_probs)
rf_auc = roc_auc_score(Y_test, rf_probs)

print('Random (chance) Prediction: AUROC = %.3f' % (r_auc))
print('Random Forest: AUROC = %.3f' % (rf_auc))

from sklearn.ensemble import RandomForestRegressor

regressor = RandomForestClassifier(max_features=3, n_estimators=5)
regressor.fit(X_train, Y_train)
y_pred = regressor.predict(X_test)
# print(y_pred)
from sklearn.metrics import accuracy_score


print("Accuracy",accuracy_score(Y_test, y_pred)*100)




r_fpr, r_tpr, _ = roc_curve(Y_test, r_probs)
rf_fpr, rf_tpr, _ = roc_curve(Y_test, rf_probs)


import matplotlib.pyplot as plt

plt.plot(r_fpr, r_tpr, linestyle='--', label='Random prediction (AUROC = %0.3f)' % r_auc)
# plt.plot(r_fpr, r_tpr, linestyle='--', label='Random prediction (AUROC = %0.3f)'% auc_score)

plt.plot(rf_fpr, rf_tpr, marker='.', label='Random Forest (AUROC = %0.3f)' % rf_auc)


# Title
plt.title('ROC Plot')
# Axis labels
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
# Show legend
plt.legend() # 
# Show plot
plt.show()

import pandas as pd

rwinedf = pd.read_csv(r'C:\Users\W10\Desktop\is 733 dATAMINING\Homework 2\red_wine.csv')

x_df = rwinedf[['citric acid','sulphates','alcohol']]
y_df = rwinedf['type'].astype('category').cat.codes
print(x_df)
print(y_df)

x_a = np.array(x_df)
y_a = np.array(y_df)
print(x_a)
print(y_a)

# Q2 One R
import numpy as np


def get_feature_quartiles(X):
    X_discretized = X.copy()
    for col in range(X.shape[1]):
        for q, class_label in zip([1.0, 0.75, 0.5, 0.25], [3, 2, 1, 0]):
            threshold = np.quantile(X[:, col], q=q)
            X_discretized[X[:, col] <= threshold, col] = class_label
    return X_discretized.astype(np.int)

Xd = get_feature_quartiles(x_a)
print(Xd)


from sklearn.model_selection import train_test_split


Xd_train, Xd_test, y_train, y_test = train_test_split(Xd, y_a, random_state=0, stratify=y_a)

from mlxtend.classifier import OneRClassifier
oner = OneRClassifier()

oner.fit(Xd_train, y_train);

oner.feature_idx_

oner.prediction_dict_

oner.predict(Xd_train)


y_pred = oner.predict(Xd_train)
train_acc = np.mean(y_pred == y_train)  
print(f'Training accuracy {train_acc*100:.2f}%')

test_acc = oner.score(Xd_test, y_test)
print(f'Test accuracy {test_acc*100:.2f}%')


SVM model

from sklearn import svm, datasets
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.datasets import make_classification
from sklearn import datasets
from sklearn.datasets import fetch_openml
mice = fetch_openml(name = 'red_wine',version=2)
import numpy as np
import sklearn
# from sklearn.datasets import fetch_rcv1
# rcv1 = fetch_rcv1()


X = mice.data
type_map = {'high': 1, 'low': 0}
mice.data['type'] = mice.data['type'].map(type_map)
Y = mice.data.pop('type')

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=0)

clf = svm.SVC(kernel='linear', C=3).fit(X_train, y_train)

classifier_predictions = clf.predict(X_test)
print("Accuracy",accuracy_score(y_test, classifier_predictions)*100)


SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto',probability=True)
SVM.fit(X_train,y_train)
# predict the labels on validation dataset
predictions_SVM = SVM.predict(X_test)
# Use accuracy_score function to get the accuracy
print("SVM Accuracy Score -> ",accuracy_score(predictions_SVM, y_test))


# X = dataset.data
# type_map = {'high': 1, 'low': 0}
# dataset.data['type'] = dataset.data['type'].map(type_map)
# Y = dataset.data.pop('type')

probs = SVM.predict_proba(X_test)
preds = probs[:,1]
fpr, tpr, threshold = roc_curve(y_test, preds)
print("SVM Area under curve -> ",auc(fpr, tpr))








    
    
